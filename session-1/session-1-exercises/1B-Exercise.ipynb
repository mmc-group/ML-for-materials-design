{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b19353-6e8b-4106-8b25-698f90c3e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports: \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b75b5-60af-40a0-ac90-6ba27fc97c6f",
   "metadata": {},
   "source": [
    "## Note: Fill in at places marked with \"To-do\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58d322-3d44-4a79-a66a-777b05a89acf",
   "metadata": {},
   "source": [
    "We will use gradient-based optimization to find roots of the quadratic function:  $$f(a)=a^2-2a+1=0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cb672-80f5-4409-afae-987be3916e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate 'lr' with value 0.1\n",
    "#To-do\n",
    "\n",
    "# Here's an empty list to store the value of loss at every iteration\n",
    "loss_history = []\n",
    "\n",
    "# Initialize 'a' as a tensor of dimension 1, and initial guess value of 0.\n",
    "#To-do\n",
    "\n",
    "# Since, we want to find the value of a using optimization, set .requires_grad to True for a\n",
    "#To-do\n",
    "\n",
    "# begin iterating over 1000 steps:\n",
    "for i in range(1000):\n",
    "    \n",
    "    # predict f:\n",
    "    #To-do\n",
    "    \n",
    "    # Compute the loss. Use the squared value of f as loss. \n",
    "    #To-do\n",
    "    \n",
    "    # store the loss in a list using loss_history.append(). Use loss.detach().item() to only store the numerical value\n",
    "    #To-do\n",
    "    \n",
    "    # Call .backward() on loss to compute gradient: d_loss/d_a\n",
    "    #To-do\n",
    "    \n",
    "    # Turn off gradient tracking to not connect graph starting from 'a' back to 'a'\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # update the value of a using: lr * a.grad\n",
    "        #To-do\n",
    "    \n",
    "        # Remove any gradient information stored in 'a' using .grad.zero_ for future iterations \n",
    "        #To-do\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a08b16-321c-44dc-a75a-a54341d44830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21083e7-75e4-430c-9307-cd6adcad3d1b",
   "metadata": {},
   "source": [
    "***Root of the polynomial***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ce65c-5125-4e8e-b4bc-cc5a8a693fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a) #The answer should be close to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdf09e-d07d-4bf9-af83-6d1be8099cbd",
   "metadata": {},
   "source": [
    "## Now try the above problem with different learning rates (very small to very large) and initial guesses and discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6ad6d-34f2-435f-bea2-c2ee7daa0fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
