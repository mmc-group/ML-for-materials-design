{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac90859-d183-4f44-aab3-eb80dc8f3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:  \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da2739-c3c3-4b37-a204-633eedd53e97",
   "metadata": {},
   "source": [
    "# Underfitting vs. overfitting: role of model complexity and data amount\n",
    "\n",
    "----\n",
    "\n",
    "### Create a polynomial fitting model: $y=\\alpha_0 + \\alpha_1 x + \\alpha_2 x^2 + \\dots + \\alpha_{d-1} x^{d-1} + x^d$\n",
    "\n",
    "----\n",
    "\n",
    "### For your convenience, the following function run(...) encapsulates all the codes from the previous workbook into one single function with parameters: \n",
    "- func\n",
    "- d\n",
    "- lr\n",
    "- n_train\n",
    "- n_test\n",
    "\n",
    "### run() returns two outputs:\n",
    "- final_train_loss (loss on train data after last epoch)\n",
    "- final_test_loss (loss on test data after last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c04d0-33ac-4880-be12-6c5fd9075f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(func, d, lr, num_epochs, n_train, n_test):\n",
    "\n",
    "    # Generate training data\n",
    "    # Make sure that number that data is arranged ROW-wise\n",
    "    x_train = torch.linspace(start=0.,end=1.,steps=n_train).unsqueeze(-1)\n",
    "    y_train = func(x_train)\n",
    "    \n",
    "    # Generate test data\n",
    "    # Make sure that number that data is arranged ROW-wise\n",
    "    x_test = torch.linspace(start=0.,end=1.,steps=n_test).unsqueeze(-1)\n",
    "    y_test = func(x_test)\n",
    "    \n",
    "    # Create a function to make a prediction given some alpha and x\n",
    "    def model(alpha, x):\n",
    "        y = x**d\n",
    "        for i in range(d):\n",
    "            y += alpha[i] * (x**i)\n",
    "        return y\n",
    "    \n",
    "    # Polynomial coefficients: these are your unknowns that need to be estimated based on the data\n",
    "    alpha = torch.zeros(d)\n",
    "    \n",
    "    # Define an empty list to store loss history\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    # Since, we want to find the value of alpha using optimization, set .requires_grad to True for alpha\n",
    "    alpha.requires_grad = True\n",
    "\n",
    "    # Optimizer: use Adam\n",
    "    optimizer = torch.optim.Adam([alpha], lr=lr)\n",
    "\n",
    "    # Loss function: use the MEAN SQUARED ERROR (MSE) as loss\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    # begin iterating over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Make your predictions, USE TRAINING DATA ONLY!!\n",
    "        y_train_pred = model(alpha, x_train)\n",
    "\n",
    "        # Compute the loss. \n",
    "        train_loss = loss_function(y_train_pred, y_train)\n",
    "\n",
    "        # store the loss in a list\n",
    "        train_loss_history.append(train_loss.detach().item())\n",
    "\n",
    "        # Call .backward() on loss to compute gradient (d_loss/d_a)\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update the model paramaeters (in this case: alpha)\n",
    "        # No need for torch.no_grad() anymore with in-built optimizers\n",
    "        optimizer.step()\n",
    "\n",
    "        # remove any pre-exisitng gradients stored\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We also track the loss on test data but don't use that information for training (just for monitoring purpose)\n",
    "        with torch.no_grad(): #torch.no_grad() is back because we don't train on test data and so don't need gradients\n",
    "\n",
    "            # Make your predictions\n",
    "            y_test_pred = model(alpha, x_test)\n",
    "\n",
    "            # Compute the loss. \n",
    "            test_loss = loss_function(y_test_pred, y_test)\n",
    "\n",
    "            # store the loss in a list\n",
    "            test_loss_history.append(test_loss.detach().item())\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(train_loss_history)\n",
    "    ax.plot(test_loss_history)\n",
    "    ax.legend(['Train-pred loss','Test-pred loss'])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1e-4,10)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_train, y_train,linestyle = 'None',marker='*')\n",
    "    ax.plot(x_test, y_test)\n",
    "    ax.plot(x_test, y_test_pred)\n",
    "    ax.legend(['Train','Test-true','Test-pred'])\n",
    "    fig.suptitle('Polynomial degree: d={}'.format(d), fontsize=18)\n",
    "    # print('alpha:',list(alpha.detach()))\n",
    "    final_train_loss = train_loss_history[-1]\n",
    "    final_test_loss = test_loss_history[-1]\n",
    "    return final_train_loss, final_test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332916b6-cbf8-41b6-bc99-94cd60cc7af2",
   "metadata": {},
   "source": [
    "### Here's the function we are trying to learn. The example from the previous workbook is already coded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f2316-46ca-4e8d-87cb-8806c043bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden function that we want to learn\n",
    "def func(x):\n",
    "    pi = np.pi\n",
    "    z = torch.sin(2*pi * x)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191efac-2e2b-4285-843a-131d2fa9cc25",
   "metadata": {},
   "source": [
    "### Use run() and func() to explore the effect of model complexity on quality of the fit. In the case of polynomial regression, model complexity is given by the polynomial degree.\n",
    "\n",
    "- To get started, use lr=0.01, num_epochs=10000, n_train=6, n_test=100, and try different values of 'd'. We will discuss your observations after the exercise.\n",
    "- Once you have made your observations and time permitting, start playing with other parameters and functions (in func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc35d83-f089-43fc-a70a-481f8d2f10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_loss, final_test_loss = run(func=func, d=?, lr=?, num_epochs=?, n_train=?, n_test=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1fadd-89d4-4e3a-b4a6-b72435eb6154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
